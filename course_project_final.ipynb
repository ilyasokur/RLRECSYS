{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dimension, mu=0, theta=0.15, sigma=0.3):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class TwoLayersNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, layer1_dim, layer2_dim, output_dim, output_tanh):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, layer1_dim)\n",
    "        self.layer2 = nn.Linear(layer1_dim, layer2_dim)\n",
    "        self.layer3 = nn.Linear(layer2_dim, output_dim)\n",
    "        self.output_tanh = output_tanh\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden = self.layer1(input)\n",
    "        hidden = self.relu(hidden)\n",
    "        hidden = self.layer2(hidden)\n",
    "        hidden = self.relu(hidden)\n",
    "        output = self.layer3(hidden)\n",
    "        \n",
    "        if self.output_tanh:\n",
    "            return self.tanh(output)\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "        \n",
    "class DDPG():\n",
    "    def __init__(self, state_dim, action_dim, action_scale, noise_decrease,\n",
    "                 gamma=0.99, batch_size=64, q_lr=1e-3, pi_lr=1e-4, tau=1e-2, memory_size=100000):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_scale = action_scale\n",
    "        self.pi_model = TwoLayersNeuralNetwork(self.state_dim, 400, 300, self.action_dim, output_tanh=True)\n",
    "        self.q_model = TwoLayersNeuralNetwork(self.state_dim + self.action_dim, 400, 300, 1, output_tanh=False)\n",
    "        self.pi_target_model = deepcopy(self.pi_model)\n",
    "        self.q_target_model = deepcopy(self.q_model)\n",
    "        self.noise = OUNoise(self.action_dim)\n",
    "        self.noise_threshold = 1\n",
    "        self.noise_decrease = noise_decrease\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.q_optimazer = torch.optim.Adam(self.q_model.parameters(), lr=q_lr)\n",
    "        self.pi_optimazer = torch.optim.Adam(self.pi_model.parameters(), lr=pi_lr)\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        pred_action = self.pi_model(torch.FloatTensor(state)).detach().numpy()\n",
    "        action = self.action_scale * (pred_action + self.noise_threshold * self.noise.sample())\n",
    "        return np.clip(action, -self.action_scale, self.action_scale)\n",
    "    \n",
    "    def update_target_model(self, target_model, model, optimazer, loss):\n",
    "        optimazer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimazer.step()\n",
    "        for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_((1 - self.tau) * target_param.data + self.tau * param.data) \n",
    "    \n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, done, next_state])\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
    "            rewards = rewards.reshape(self.batch_size, 1)\n",
    "            dones = dones.reshape(self.batch_size, 1)\n",
    "            \n",
    "            pred_next_actions = self.action_scale * self.pi_target_model(next_states)\n",
    "            next_states_and_pred_next_actions = torch.cat((next_states, pred_next_actions), dim=1)\n",
    "            targets = rewards + self.gamma * (1 - dones) * self.q_target_model(next_states_and_pred_next_actions)\n",
    "            \n",
    "            states_and_actions = torch.cat((states, actions), dim=1)\n",
    "            temp = (self.q_model(states_and_actions) - targets.detach())\n",
    "            q_loss = torch.mean((targets.detach() - self.q_model(states_and_actions)) ** 2)\n",
    "            self.update_target_model(self.q_target_model, self.q_model, self.q_optimazer, q_loss)\n",
    "            \n",
    "            pred_actions = self.action_scale * self.pi_model(states)\n",
    "            states_and_pred_actions = torch.cat((states, pred_actions), dim=1)\n",
    "            pi_loss = - torch.mean(self.q_model(states_and_pred_actions))\n",
    "            self.update_target_model(self.pi_target_model, self.pi_model, self.pi_optimazer, pi_loss)\n",
    "            \n",
    "        if self.noise_threshold > 0:\n",
    "            self.noise_threshold = max(0, self.noise_threshold - self.noise_decrease)\n",
    "\n",
    "\n",
    "class MovieLensEnv:\n",
    "    def __init__(self, ratings_data):\n",
    "        self.ratings_data = ratings_data\n",
    "        self.users = ratings_data['userId'].unique()\n",
    "        self.movies = ratings_data['movieId'].unique()\n",
    "        self.state_dim = num_users + num_movies\n",
    "        self.action_dim = num_movies\n",
    "        self.current_user = None\n",
    "        self.current_movie = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_user = np.random.choice(self.users)\n",
    "        available_movies = self.ratings_data[self.ratings_data['userId'] == self.current_user]['movieId'].values\n",
    "        self.current_movie = np.random.choice(available_movies)\n",
    "        return self.preprocess_state(self.current_user, self.current_movie)\n",
    "\n",
    "    def step(self, action):\n",
    "        selected_movie_id = np.argmax(action)\n",
    "        if selected_movie_id in self.movies:\n",
    "            self.current_movie = selected_movie_id\n",
    "        reward = self.get_reward()\n",
    "        next_state = self.preprocess_state(self.current_user, self.current_movie)\n",
    "        done = True \n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_reward(self):\n",
    "        rating = self.ratings_data[(self.ratings_data['userId'] == self.current_user) & \n",
    "                                   (self.ratings_data['movieId'] == self.current_movie)]['rating'].values\n",
    "        return (rating.item() - 2.5) / 2.5 if len(rating) > 0 else -1\n",
    "\n",
    "    def sample_action(self):\n",
    "        action = np.zeros(self.action_dim)\n",
    "        action[np.random.choice(self.action_dim)] = 1\n",
    "        return action\n",
    "\n",
    "    def preprocess_state(self, user_id, movie_id):\n",
    "        state = np.zeros(num_users + num_movies)\n",
    "        state[user_to_index[user_id]] = 1\n",
    "        state[num_users + movie_to_index[movie_id]] = 1\n",
    "        return state\n",
    "\n",
    "ratings_data = pd.read_csv('ratings.csv')\n",
    "\n",
    "user_ids = ratings_data['userId'].unique()\n",
    "user_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "movie_ids = ratings_data['movieId'].unique()\n",
    "movie_to_index = {movie_id: index for index, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "\n",
    "env = MovieLensEnv(ratings_data)\n",
    "max_episodes = 100\n",
    "max_steps = 1000\n",
    "\n",
    "\n",
    "ddpg_agent = DDPG(state_dim=env.state_dim, action_dim=env.action_dim, action_scale=2, noise_decrease=0.0001)\n",
    "episode_rewards = []\n",
    "hit_rates = []\n",
    "\n",
    "for episode in tqdm(range(max_episodes)):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    hit_count = 0\n",
    "    total_recommendations = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = ddpg_agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if reward > 0:\n",
    "            hit_count += 1\n",
    "\n",
    "        total_recommendations += 1\n",
    "\n",
    "        ddpg_agent.fit(state, action, reward, done, next_state)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    hit_rate = hit_count / total_recommendations if total_recommendations > 0 else 0\n",
    "    hit_rates.append(hit_rate)\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "\n",
    "print(np.mean(hit_rates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "class SAC(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=1e-3, tau=1e-2, \n",
    "                 batch_size=64, pi_lr=1e-3, q_lr=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi_model = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 2 * action_dim), nn.Tanh())\n",
    "\n",
    "        self.q1_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.q2_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "\n",
    "        self.pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), pi_lr)\n",
    "        self.q1_optimizer = torch.optim.Adam(self.q1_model.parameters(), q_lr)\n",
    "        self.q2_optimizer = torch.optim.Adam(self.q2_model.parameters(), q_lr)\n",
    "        self.q1_target_model = deepcopy(self.q1_model)\n",
    "        self.q2_target_model = deepcopy(self.q2_model)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, _ = self.predict_actions(state)\n",
    "        return action.squeeze(1).detach().numpy()\n",
    "\n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, done, next_state])\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
    "            rewards, dones = rewards.unsqueeze(1), dones.unsqueeze(1)\n",
    "\n",
    "            next_actions, next_log_probs = self.predict_actions(next_states)\n",
    "            next_states_and_actions = torch.cat((next_states, next_actions), dim=1)\n",
    "            next_q1_values = self.q1_target_model(next_states_and_actions)\n",
    "            next_q2_values = self.q2_target_model(next_states_and_actions)\n",
    "            next_min_q_values = torch.min(next_q1_values, next_q2_values)\n",
    "            targets = rewards + self.gamma * (1 - dones) * (next_min_q_values - self.alpha * next_log_probs)\n",
    "\n",
    "            states_and_actions = torch.cat((states, actions), dim=1)\n",
    "            q1_loss = torch.mean((self.q1_model(states_and_actions) - targets.detach()) ** 2)\n",
    "            q2_loss = torch.mean((self.q2_model(states_and_actions) - targets.detach()) ** 2)\n",
    "            self.update_model(q1_loss, self.q1_optimizer, self.q1_model, self.q1_target_model)\n",
    "            self.update_model(q2_loss, self.q2_optimizer, self.q2_model, self.q2_target_model)\n",
    "\n",
    "            pred_actions, log_probs = self.predict_actions(states)\n",
    "            states_and_pred_actions = torch.cat((states, pred_actions), dim=1)\n",
    "            q1_values = self.q1_model(states_and_pred_actions)\n",
    "            q2_values = self.q2_model(states_and_pred_actions)\n",
    "            min_q_values = torch.min(q1_values, q2_values)\n",
    "            pi_loss = - torch.mean(min_q_values - self.alpha * log_probs)\n",
    "            self.update_model(pi_loss, self.pi_optimizer)\n",
    "            \n",
    "    def update_model(self, loss, optimizer, model=None, target_model=None):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if model != None and target_model != None:\n",
    "            for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "                new_target_param = (1 - self.tau) * target_param + self.tau * param\n",
    "                target_param.data.copy_(new_target_param)\n",
    "\n",
    "    def predict_actions(self, states):\n",
    "        means, log_stds = self.pi_model(states).chunk(2, dim=1)\n",
    "        dists = Normal(means, torch.exp(log_stds))\n",
    "        actions = dists.rsample()\n",
    "        log_probs = dists.log_prob(actions)\n",
    "        return actions, log_probs\n",
    "    def train_model(self, env, max_episodes=100, max_steps=1000, batch_size=64, warmup_steps=1000):\n",
    "        episode_rewards = []\n",
    "        hit_rates = []\n",
    "        for episode in range(max_episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            for step in range(max_steps):\n",
    "                if step < warmup_steps:\n",
    "                    action = env.sample_action()\n",
    "                else:\n",
    "                    action = self.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                self.fit(state, action, reward, done, next_state)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "            episode_rewards.append(episode_reward)\n",
    "            hit_rate = hitrate(self, env)\n",
    "            hit_rates.append(hit_rate)\n",
    "        return episode_rewards, hit_rates\n",
    "\n",
    "def hitrate(model, env, num_trials=1000):\n",
    "    hits = 0\n",
    "    for _ in range(num_trials):\n",
    "        user_id = random.choice(env.users)\n",
    "        user_state = np.zeros(env.state_dim)\n",
    "        user_state[user_to_index[user_id]] = 1\n",
    "        recommended_movie_id = np.argmax(model.get_action(user_state))\n",
    "        if recommended_movie_id in env.ratings_data[env.ratings_data['userId'] == user_id]['movieId'].values:\n",
    "            hits += 1\n",
    "    return hits / num_trials\n",
    "\n",
    "\n",
    "env = MovieLensEnv(ratings_data)\n",
    "\n",
    "sac_model = SAC(state_dim=env.state_dim, action_dim=env.action_dim)\n",
    "episode_rewards, hit_rates = sac_model.train_model(env, max_episodes=100, max_steps=1000, batch_size=64, warmup_steps=3000)\n",
    "\n",
    "print(np.max(hit_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
